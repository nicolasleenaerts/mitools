# Add MI of variables of the previous depth level
if (depth_level >2){
if (depth_level==4){break}
search_df = merge(x = search_df, y = dplyr::select(subset(pairs_retained_df,depth==(depth_level-1)),'mi','pair'), by.x = 'X2',by.y = 'pair', all.x = TRUE)
search_df$mi_diff = search_df$mi_pairs - search_df$mi
}
else{
search_df$mi_diff = 0
}
# Give pairs names in alphabetical order
search_df$pair_names = paste(search_df$X1,search_df$X2,sep = sep)
search_df$pair_names = apply(search_df['pair_names'], 1, function(x) paste(sort(strsplit(x,split=sep,fixed=T)[[1]]),collapse=sep))
# Remove duplicates
search_df=search_df[!duplicated(search_df$pair_names),]
# Add retained pairs to retained pairs df
current_rows=nrow(pairs_retained_df)
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'depth'] = depth_level
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'pair'] = search_df$pair_names
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'mi'] = search_df$mi_pairs
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'relative.mi'] = search_df$mi_diff
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'p'] = search_df$p
# Add pairs to the original data
new_data = as.data.frame(apply(search_df,1,function(x) data[x[1]]*data[x[2]]))
colnames(new_data)=search_df$pair_names
data[,(ncol(data)+1):(ncol(data)+nrow(search_df))]=new_data
# Update progress bar
setTxtProgressBar(pb,(depth_level-1))
}
# Close progress bar
close(pb)
# Create results list
results_list = list()
# Store the rest of the results
results_list$expanded.data = data
results_list$original.variables = orig_variables
results_list$pairs = pairs_retained_df
# Return results
return(results_list)
}
results = pairmi(misimdata[,1:6],p.threshold = 0.05,depth = 5)
View(results$pairs)
pairmi(misimdata[,2:6])
#' @param sep A string which will be used to seperate the variables in the pair name
#'
#' @return A list that includes
#' (1) A data.frame object with the original data and the data of the significant sets
#' (2) A vector with the orignal variable names
#' (3) A data.frame object with information on the significant sets
#' @export
#'
#' @examples
#' pairmi(misimdata[,2:6])
pairmi <- function(data,alpha=0.05,MI.threshold=NULL,n_elements=5,sep='_'){
# Create df to store information about the pairs
pairs_retained_df = data.frame(matrix(ncol=5))[-1,]
colnames(pairs_retained_df) = c('n_elements','set','mi','relative.mi','p')
# Get original variables
orig_variables = colnames(data)
# Create progress bar
message('Pairing Data')
pb = txtProgressBar(min = 0, max = (n_elements-1), initial = 0)
# Create Pairs
for (depth_level in 2:n_elements){
# Reduce the number of variables to pair
if (depth_level>2) {search_data=dplyr::select(data,c(dplyr::all_of(orig_variables),pairs_retained_df$set[pairs_retained_df$n_elements==(depth_level-1)]))
} else {search_data=data}
# Get all possible combinations of the variables
search_df = data.frame(t(combn(colnames(search_data), 2, simplify=TRUE)))
# Retain only correct combinations if depth level >2
if (depth_level >2){
# Only retain pairs with a variable of the original data and a variable of the previous depth level
search_df$orig_variable_in_pair = ifelse(search_df$X1%in%dplyr::all_of(orig_variables)|search_df$X2%in%dplyr::all_of(orig_variables),1,0)
search_df$highest_depth_in_pair = ifelse(search_df$X1%in%pairs_retained_df$set[pairs_retained_df$n_elements==(depth_level-1)]|search_df$X2%in%pairs_retained_df$set[pairs_retained_df$n_elements==(depth_level-1)],1,0)
search_df$retain = ifelse(search_df$orig_variable_in_pair==1&search_df$highest_depth_in_pair==1,1,0)
search_df = subset(search_df,retain==1)
# Remove pairs with overlap (i.e., that an variable of the original data is also included in the variable of the previous depth level)
search_df=subset(search_df,apply(search_df,1,function(x) grepl(paste0('\\b',x[1],'\\b'),strsplit(x[2],split=sep,fixed=T))==F))
}
# Calculate mutual information values of possible pairs
search_df$mi_pairs = apply(search_df,1,function(x) mi(unlist(search_data[x[1]]),unlist(search_data[x[2]])))
# Add the joint counts of the possible pairs
search_df$joint_counts = apply(search_df,1,function(x) sum(unlist(search_data[x[1]])*(unlist(search_data[x[2]])),na.rm = T))
# Calculate G statistic
# If the percentage of joint counts is > 50% of the total number of observations, then the joint counts are flipped
# Such that we apply the less frequent of the 0 vs. 1 effects to estimate power
search_df$joint_counts_correct = ifelse(search_df$joint_counts > .5*(nrow(search_data)), nrow(search_data) - search_df$joint_counts, search_df$joint_counts)
search_df$g = 2*(search_df$joint_counts_correct) * search_df$mi_pairs
search_df$p = pchisq(search_df$g, 1, lower.tail=FALSE)
# Only keep the pairs which the user wants to retain
if (is.null(MI.threshold)==T){
search_df = subset(search_df,p<alpha)
}
else{
search_df = subset(search_df,mi_pairs>MI.threshold)
}
# Stop if no pairs are retained
if (nrow(search_df)==0){message(paste('stopped at max number of elements:',(depth_level-1)))
break}
# Add MI of variables of the previous depth level
if (depth_level >2){
if (depth_level==4){break}
search_df = merge(x = search_df, y = dplyr::select(subset(pairs_retained_df,n_elements==(depth_level-1)),'mi','set'), by.x = 'X2',by.y = 'set', all.x = TRUE)
search_df$mi_diff = search_df$mi_pairs - search_df$mi
}
else{
search_df$mi_diff = 0
}
# Give pairs names in alphabetical order
search_df$pair_names = paste(search_df$X1,search_df$X2,sep = sep)
search_df$pair_names = apply(search_df['pair_names'], 1, function(x) paste(sort(strsplit(x,split=sep,fixed=T)[[1]]),collapse=sep))
# Remove duplicates
search_df=search_df[!duplicated(search_df$pair_names),]
# Add retained pairs to retained pairs df
current_rows=nrow(pairs_retained_df)
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'n_elements'] = depth_level
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'set'] = search_df$pair_names
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'mi'] = search_df$mi_pairs
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'relative.mi'] = search_df$mi_diff
pairs_retained_df[(current_rows+1):(current_rows+nrow(search_df)),'p'] = search_df$p
# Add pairs to the original data
new_data = as.data.frame(apply(search_df,1,function(x) data[x[1]]*data[x[2]]))
colnames(new_data)=search_df$pair_names
data[,(ncol(data)+1):(ncol(data)+nrow(search_df))]=new_data
# Update progress bar
setTxtProgressBar(pb,(depth_level-1))
}
# Close progress bar
close(pb)
# Create results list
results_list = list()
# Store the rest of the results
results_list$expanded.data = data
results_list$original.variables = orig_variables
results_list$sets = pairs_retained_df
# Return results
return(results_list)
}
results = pairmi(misimdata[,1:6],alpha = 0.05,n_elements = 5)
View(results$sets)
View(sets)
View(results$sets)
setmapmi(pairmiresults$orig.variables,pairmiresults$pairs,2)
#' @param original.variables The names of the original variables that were paired up (Vector of strings)
#' @param sets The information on the sets from the pairmi function (Data.frame object)
#' @param n_elements The depth of sets that you want to visualize (Integer)
#'
#' @return A setmap showing which original variables make up the sets at a certain depth
#' @import dplyr
#' @export
#'
#' @examples
#' setmapmi(pairmiresults$orig.variables,pairmiresults$pairs,2)
setmapmi <- function(original.variables = NULL,sets=NULL,n_elements=NULL){
# Load dplyr
require(dplyr)
# Extract pairs from the requested pair level
sets = subset(sets,sets$n_elements==n_elements)
# Split pairs into sets
combos = data.frame(pairs['set'] %>% rowwise() %>% mapply(grepl,orig.variables,.))
combos = apply(combos,1, function(x) list(names(which(x))))
combos = lapply(combos, "[[", 1)
# Create Venn object
sets = RVenn::Venn(combos)
# Create setmap
return(RVenn::setmap(sets))
}
setmapmi(results$original.variables,results$sets,2)
#' @param original.variables The names of the original variables that were paired up (Vector of strings)
#' @param sets The information on the sets from the pairmi function (Data.frame object)
#' @param n_elements The depth of sets that you want to visualize (Integer)
#'
#' @return A setmap showing which original variables make up the sets at a certain depth
#' @import dplyr
#' @export
#'
#' @examples
#' setmapmi(pairmiresults$orig.variables,pairmiresults$pairs,2)
setmapmi <- function(original.variables = NULL,sets=NULL,n_elements=NULL){
# Load dplyr
require(dplyr)
# Extract pairs from the requested pair level
sets = subset(sets,sets$n_elements==n_elements)
# Split pairs into sets
combos = data.frame(sets['set'] %>% rowwise() %>% mapply(grepl,orig.variables,.))
combos = apply(combos,1, function(x) list(names(which(x))))
combos = lapply(combos, "[[", 1)
# Create Venn object
sets = RVenn::Venn(combos)
# Create setmap
return(RVenn::setmap(sets))
}
#' @param original.variables The names of the original variables that were paired up (Vector of strings)
#' @param sets The information on the sets from the pairmi function (Data.frame object)
#' @param n_elements The depth of sets that you want to visualize (Integer)
#'
#' @return A setmap showing which original variables make up the sets at a certain depth
#' @import dplyr
#' @export
#'
#' @examples
#' setmapmi(pairmiresults$orig.variables,pairmiresults$pairs,2)
setmapmi <- function(original.variables = NULL,sets=NULL,n_elements=NULL){
# Load dplyr
require(dplyr)
# Extract pairs from the requested pair level
sets = subset(sets,sets$n_elements==n_elements)
# Split pairs into sets
combos = data.frame(sets['set'] %>% rowwise() %>% mapply(grepl,orig.variables,.))
combos = apply(combos,1, function(x) list(names(which(x))))
combos = lapply(combos, "[[", 1)
# Create Venn object
sets = RVenn::Venn(combos)
# Create setmap
return(RVenn::setmap(sets))
}
setmapmi(results$original.variables,results$sets,2)
probstat(misimdata$y,results$expanded.data,nfolds = 5)
setmapmi(results$original.variables,results$sets,2)
#### Load necessary libraries and functions ####
source('/Users/u0127988/Documents/GitHub/NLML/Elastic Net/elastic_net_wrapper.R')
source('/Users/u0127988/Documents/GitHub/NLML/Elastic Net/Multiple Imputation/elastic_net_wrapper_mi.R')
source('~/Documents/GitHub/NLML/Preprocessing/mldataprepper.R')
simstudy_wrapper_kolar <- function(outcome_percentages=c(0.01,0.1,0.3),n_sample=c(2500,5000),n_predictors =c(3,6,12,24),percentage_continuous=c(0.25,0.5,0.75),seed=404,
missingness=c('MCAR','MAR'), missingness_percentage=c(0.05,0.10),max_effect_sizes=c(0.8)){
# set functions
require(simstudy)
`%!in%` = Negate(`%in%`)
# Get all possible combinations
combinations = expand.grid(outcome_percentages=outcome_percentages,n_sample=n_sample,n_predictors=n_predictors,
percentage_continuous=percentage_continuous,missingness=missingness,
missingness_percentage=missingness_percentage,max_effect_sizes=max_effect_sizes)
# Create data set list
results = list()
# Loop over combinations
for (row in 1:nrow(combinations)){
# Create strings for predictors
predictor_strings = sapply(c(1:combinations[row,'n_predictors']),function (x) paste0('x',x))
# Select continuous predictors
set.seed(seed)
predictor_strings_con = sample(predictor_strings,combinations[row,'percentage_continuous']*length(predictor_strings))
predictor_strings_cat = predictor_strings[predictor_strings%!in%predictor_strings_con]
# Set effect sized
effect_sizes_con = seq(-combinations[row,'max_effect_sizes'],combinations[row,'max_effect_sizes'],length.out=length(predictor_strings_con))
effect_sizes_cat = seq(-combinations[row,'max_effect_sizes'],combinations[row,'max_effect_sizes'],length.out=length(predictor_strings_cat))
# Set outcome
def = defData(varname = "y", dist = "binary", formula = combinations[row,'outcome_percentages'])
# Set continuous predictors
if (length(predictor_strings_con)>0){
for (i in 1:length(predictor_strings_con)) {
set.seed(i)
formula = paste0(round(rnorm(1),2),'+y*',effect_sizes_con[i])
def = defData(def, varname = predictor_strings_con[i], dist = "normal",formula = formula,variance = 1)
}
}
# Set categoric predictors
if (length(predictor_strings_cat)>0){
for (i in 1:length(predictor_strings_cat)) {
set.seed(i)
formula = paste0(abs(round(rnorm(1,sd=0.5),2)),'+y*',effect_sizes_cat[i])
def = defData(def, varname = predictor_strings_cat[i], dist = "binary",formula = formula,link = "logit")
}
}
# Set seed
set.seed(seed)
# Generate data
simulated_data = genData(combinations[row,'n_sample'], def)
# store data
results[[row]] = list(simulated_data)
# Generate missingness
if (is.null(missingness)==F){
# Calculate missingness percentage
p_missing = missingness_percentage
# Set missingness outcome
if (combinations[row,'missingness']=='MCAR'){
defM = defMiss(varname = "y", formula = p_missing, logit.link = FALSE)
}
else if (combinations[row,'missingness']=='MAR'){
formula = paste0(2*round(p_missing,3)/3,'+',(round(p_missing,3))/3,'*', sample(predictor_strings_cat,1))
defM = defMiss(varname = 'y', formula = formula, logit.link = FALSE)
}
# Generate missing data set
set.seed(row)
missMat = genMiss(simulated_data, defM, idvars = "id")
missing_data = genObs(simulated_data, missMat, idvars = "id")
# store data
results[[row]][[2]] = missing_data
}
# store combination
results[[row]][[3]] = combinations[row,]
}
# return results
return(results)
}
library(xgboost)
library(ParBayesianOptimization)
#### Simulate data ####
simulated_data_list <- simstudy_wrapper_kolar(outcome_percentages=c(0.3),n_sample=c(2500),n_predictors =c(20),
percentage_continuous=c(0.80),seed=404,missingness=c('MAR'),
missingness_percentage=c(0.50),max_effect_sizes=c(0.8))
misimdata <- data.frame(simulated_data_list[[1]][[1]])
View(misimdata)
misimdata <- misimdata[,c(2:22)]
#### Simulate data ####
simulated_data_list <- simstudy_wrapper_kolar(outcome_percentages=c(0.3),n_sample=c(2500),n_predictors =c(20),
percentage_continuous=c(0),seed=404,missingness=c('MAR'),
missingness_percentage=c(0.50),max_effect_sizes=c(0.8))
### Assess effect multiple imputation on missingness # No difference here # No missingness at deployment
data_full <- data.frame(simulated_data_list[[1]][[1]])
misimdata <- data.frame(simulated_data_list[[1]][[1]])
misimdata <- misimdata[,c(2:22)]
View(misimdata)
View(misimdata)
misimdata <- misimdata[,c(1:11)]
usethis::use_data(misimdata, overwrite = TRUE)
results = pairmi(misimdata[,1:6],alpha = 0.05,n_elements = 5)
results = pairmi(misimdata[,2:11],alpha = 0.05,n_elements = 5)
#### Load necessary libraries and functions ####
source('/Users/u0127988/Documents/GitHub/NLML/Elastic Net/elastic_net_wrapper.R')
source('/Users/u0127988/Documents/GitHub/NLML/Elastic Net/Multiple Imputation/elastic_net_wrapper_mi.R')
source('~/Documents/GitHub/NLML/Preprocessing/mldataprepper.R')
simstudy_wrapper_kolar <- function(outcome_percentages=c(0.01,0.1,0.3),n_sample=c(2500,5000),n_predictors =c(3,6,12,24),percentage_continuous=c(0.25,0.5,0.75),seed=404,
missingness=c('MCAR','MAR'), missingness_percentage=c(0.05,0.10),max_effect_sizes=c(0.8)){
# set functions
require(simstudy)
`%!in%` = Negate(`%in%`)
# Get all possible combinations
combinations = expand.grid(outcome_percentages=outcome_percentages,n_sample=n_sample,n_predictors=n_predictors,
percentage_continuous=percentage_continuous,missingness=missingness,
missingness_percentage=missingness_percentage,max_effect_sizes=max_effect_sizes)
# Create data set list
results = list()
# Loop over combinations
for (row in 1:nrow(combinations)){
# Create strings for predictors
predictor_strings = sapply(c(1:combinations[row,'n_predictors']),function (x) paste0('x',x))
# Select continuous predictors
set.seed(seed)
predictor_strings_con = sample(predictor_strings,combinations[row,'percentage_continuous']*length(predictor_strings))
predictor_strings_cat = predictor_strings[predictor_strings%!in%predictor_strings_con]
# Set effect sized
effect_sizes_con = seq(-combinations[row,'max_effect_sizes'],combinations[row,'max_effect_sizes'],length.out=length(predictor_strings_con))
effect_sizes_cat = seq(-combinations[row,'max_effect_sizes'],combinations[row,'max_effect_sizes'],length.out=length(predictor_strings_cat))
# Set outcome
def = defData(varname = "y", dist = "binary", formula = combinations[row,'outcome_percentages'])
# Set continuous predictors
if (length(predictor_strings_con)>0){
for (i in 1:length(predictor_strings_con)) {
set.seed(i)
formula = paste0(round(rnorm(1),2),'+y*',effect_sizes_con[i])
def = defData(def, varname = predictor_strings_con[i], dist = "normal",formula = formula,variance = 1)
}
}
# Set categoric predictors
if (length(predictor_strings_cat)>0){
for (i in 1:length(predictor_strings_cat)) {
set.seed(i)
formula = paste0(abs(round(rnorm(1,sd=0.5),2)),'+y*',effect_sizes_cat[i],'+',sample(predictor_strings_cat,1),'*',effect_sizes_cat[i])
def = defData(def, varname = predictor_strings_cat[i], dist = "binary",formula = formula,link = "logit")
}
}
# Set seed
set.seed(seed)
# Generate data
simulated_data = genData(combinations[row,'n_sample'], def)
# store data
results[[row]] = list(simulated_data)
# Generate missingness
if (is.null(missingness)==F){
# Calculate missingness percentage
p_missing = missingness_percentage
# Set missingness outcome
if (combinations[row,'missingness']=='MCAR'){
defM = defMiss(varname = "y", formula = p_missing, logit.link = FALSE)
}
else if (combinations[row,'missingness']=='MAR'){
formula = paste0(2*round(p_missing,3)/3,'+',(round(p_missing,3))/3,'*', sample(predictor_strings_cat,1))
defM = defMiss(varname = 'y', formula = formula, logit.link = FALSE)
}
# Generate missing data set
set.seed(row)
missMat = genMiss(simulated_data, defM, idvars = "id")
missing_data = genObs(simulated_data, missMat, idvars = "id")
# store data
results[[row]][[2]] = missing_data
}
# store combination
results[[row]][[3]] = combinations[row,]
}
# return results
return(results)
}
library(xgboost)
library(ParBayesianOptimization)
#### Simulate data ####
simulated_data_list <- simstudy_wrapper_kolar(outcome_percentages=c(0.3),n_sample=c(2500),n_predictors =c(20),
percentage_continuous=c(0.80),seed=404,missingness=c('MAR'),
missingness_percentage=c(0.50),max_effect_sizes=c(0.8))
### Assess effect multiple imputation on missingness # No difference here # No missingness at deployment
data_full <- data.frame(simulated_data_list[[1]][[1]])
#### Load necessary libraries and functions ####
source('/Users/u0127988/Documents/GitHub/NLML/Elastic Net/elastic_net_wrapper.R')
source('/Users/u0127988/Documents/GitHub/NLML/Elastic Net/Multiple Imputation/elastic_net_wrapper_mi.R')
source('~/Documents/GitHub/NLML/Preprocessing/mldataprepper.R')
simstudy_wrapper_kolar <- function(outcome_percentages=c(0.01,0.1,0.3),n_sample=c(2500,5000),n_predictors =c(3,6,12,24),percentage_continuous=c(0.25,0.5,0.75),seed=404,
missingness=c('MCAR','MAR'), missingness_percentage=c(0.05,0.10),max_effect_sizes=c(0.8)){
# set functions
require(simstudy)
`%!in%` = Negate(`%in%`)
# Get all possible combinations
combinations = expand.grid(outcome_percentages=outcome_percentages,n_sample=n_sample,n_predictors=n_predictors,
percentage_continuous=percentage_continuous,missingness=missingness,
missingness_percentage=missingness_percentage,max_effect_sizes=max_effect_sizes)
# Create data set list
results = list()
# Loop over combinations
for (row in 1:nrow(combinations)){
# Create strings for predictors
predictor_strings = sapply(c(1:combinations[row,'n_predictors']),function (x) paste0('x',x))
# Select continuous predictors
set.seed(seed)
predictor_strings_con = sample(predictor_strings,combinations[row,'percentage_continuous']*length(predictor_strings))
predictor_strings_cat = predictor_strings[predictor_strings%!in%predictor_strings_con]
# Set effect sized
effect_sizes_con = seq(-combinations[row,'max_effect_sizes'],combinations[row,'max_effect_sizes'],length.out=length(predictor_strings_con))
effect_sizes_cat = seq(-combinations[row,'max_effect_sizes'],combinations[row,'max_effect_sizes'],length.out=length(predictor_strings_cat))
# Set outcome
def = defData(varname = "y", dist = "binary", formula = combinations[row,'outcome_percentages'])
# Set continuous predictors
if (length(predictor_strings_con)>0){
for (i in 1:length(predictor_strings_con)) {
set.seed(i)
formula = paste0(round(rnorm(1),2),'+y*',effect_sizes_con[i])
def = defData(def, varname = predictor_strings_con[i], dist = "normal",formula = formula,variance = 1)
}
}
# Set categoric predictors
if (length(predictor_strings_cat)>0){
for (i in 1:1) {
set.seed(i)
formula = paste0(abs(round(rnorm(1,sd=0.5),2)),'+y*',effect_sizes_cat[i])
def = defData(def, varname = predictor_strings_cat[i], dist = "binary",formula = formula,link = "logit")
}
}
# Set categoric predictors
if (length(predictor_strings_cat)>0){
for (i in 2:length(predictor_strings_cat)) {
set.seed(i)
formula = paste0(abs(round(rnorm(1,sd=0.5),2)),'+y*',effect_sizes_cat[i],'+x',(i-1),'*',effect_sizes_cat[i])
def = defData(def, varname = predictor_strings_cat[i], dist = "binary",formula = formula,link = "logit")
}
}
# Set seed
set.seed(seed)
# Generate data
simulated_data = genData(combinations[row,'n_sample'], def)
# store data
results[[row]] = list(simulated_data)
# Generate missingness
if (is.null(missingness)==F){
# Calculate missingness percentage
p_missing = missingness_percentage
# Set missingness outcome
if (combinations[row,'missingness']=='MCAR'){
defM = defMiss(varname = "y", formula = p_missing, logit.link = FALSE)
}
else if (combinations[row,'missingness']=='MAR'){
formula = paste0(2*round(p_missing,3)/3,'+',(round(p_missing,3))/3,'*', sample(predictor_strings_cat,1))
defM = defMiss(varname = 'y', formula = formula, logit.link = FALSE)
}
# Generate missing data set
set.seed(row)
missMat = genMiss(simulated_data, defM, idvars = "id")
missing_data = genObs(simulated_data, missMat, idvars = "id")
# store data
results[[row]][[2]] = missing_data
}
# store combination
results[[row]][[3]] = combinations[row,]
}
# return results
return(results)
}
library(xgboost)
library(ParBayesianOptimization)
#### Simulate data ####
simulated_data_list <- simstudy_wrapper_kolar(outcome_percentages=c(0.3),n_sample=c(2500),n_predictors =c(20),
percentage_continuous=c(0.80),seed=404,missingness=c('MAR'),
missingness_percentage=c(0.50),max_effect_sizes=c(0.8))
### Assess effect multiple imputation on missingness # No difference here # No missingness at deployment
data_full <- data.frame(simulated_data_list[[1]][[1]])
View(data_full)
#### Simulate data ####
simulated_data_list <- simstudy_wrapper_kolar(outcome_percentages=c(0.3),n_sample=c(2500),n_predictors =c(20),
percentage_continuous=c(0),seed=404,missingness=c('MAR'),
missingness_percentage=c(0.50),max_effect_sizes=c(0.8))
### Assess effect multiple imputation on missingness # No difference here # No missingness at deployment
data_full <- data.frame(simulated_data_list[[1]][[1]])
View(data_full)
misimdata <- data_full[,c(2:12)]
View(misimdata)
results = pairmi(misimdata[,2:11],alpha = 0.05,n_elements = 5)
View(results$sets)
setmapmi(results$original.variables,results$sets,2)
usethis::use_data(misimdata, overwrite = TRUE)
View(evaluated_sets)
results = pairmi(misimdata[,2:11],alpha = 0.05,n_elements = 5)
evaluated_sets = probstat(misimdata$y,results$expanded.data,nfolds = 5)
evaluated_sets
evaluated_sets = probstat(misimdata$y,results$expanded.data[,results$sets],nfolds = 5)
results$sets
evaluated_sets = probstat(misimdata$y,results$expanded.data[,results$sets$set],nfolds = 5)\
evaluated_sets = probstat(misimdata$y,results$expanded.data[,results$sets$set],nfolds = 5)
